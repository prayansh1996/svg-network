{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers.core import Activation, Dropout, Dense, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fonts:\n",
    "    def __init__(self, name, d):\n",
    "        self.name = name\n",
    "        self.d = d\n",
    "\n",
    "f = open(\"savedFonts\", 'r')\n",
    "fonts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load commands\n",
    "commands = []\n",
    "for i, font in enumerate(fonts):\n",
    "    commands.append([])\n",
    "    for command in font.d:\n",
    "        commands[i].append(command[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 't', 't', 't', 'z', 'M', 'v', 'q', 't', 't', 'h', 'v', 'h', 'v', 'h', 'q', 't', 't', 'v', 'q', 't', 't', 'h', 'v', 'q', 'h', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'l', 'q', 'q', 'q', 't', 'v', 'q', 't', 'q', 'v', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'h', 'q', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'v', 'q', 't', 'v', 'l', 'v', 'q', 't', 'v', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'q', 'q', 'v', 'q', 't', 't', 't', 'z', 'M', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 'v', 'h', 'v', 'q', 'v', 'q', 'v', 'h', 'v', 'z', 'M', 'q', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'l', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'q', 'q', 't', 'q', 't', 'z', 'M', 'q', 'q', 'q', 'l', 'q', 'q', 'q', 'q', 't', 'l', 'q', 't', 'l', 'q', 'q', 'q', 'q', 'l', 'q', 'q', 'q', 'v', 'q', 't', 'q', 'q', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 't', 't', 't', 't', 't', 't', 't', 'q', 't', 't', 't', 'z', 'M', 'l', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'h', 'v', 'h', 'v', 'h', 'v', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'q', 'h', 'l', 'h', 'q', 'h', 'q', 't', 'h', 'q', 'q', 'q', 't', 'v', 'q', 't', 'l', 'h', 'q', 't', 'q', 'q', 'h', 'q', 'h', 'h', 'q', 'q', 'q', 't', 'v', 'q', 't', 'q', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'z', 'M', 'h', 'l', 'h', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 't', 't', 't', 'z', 'M', 'l', 'q', 'q', 't', 'q', 'q', 'l', 'q', 'q', 't', 'q', 'l', 'q', 'q', 't', 'q', 'l', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'v', 'z', 'M', 'v', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 'v', 'q', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'q', 'v', 'l', 'l', 'l', 'v', 'l', 'q', 'l', 'l', 'l', 'q', 'z', 'M', 'q', 'l', 'l', 'l', 'q', 'l', 'l', 'l', 'q', 'q', 'q', 'l', 'l', 'h', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'q', 'q', 'l', 'q', 'l', 'l', 'l', 'q', 'q', 't', 'q', 'q', 'l', 'q', 'q', 'v', 'q', 'q', 'l', 'l', 'q', 'l', 'l', 'q', 'v', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'l', 'h', 'l', 'l', 'l', 'q', 'h', 'q', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'q', 't', 'q', 't', 'v', 'q', 't', 'q', 't', 'v', 'l', 'l', 'v', 'q', 't', 'q', 't', 'v', 'z', 'M', 'q', 't', 'q', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'h', 'v', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'z', 'M', 'h', 'v', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'l', 'q', 't', 'q', 't', 'q', 't', 'l', 'q', 't', 'q', 't', 'q', 't', 'l', 'q', 't', 'q', 't', 'q', 't', 'l', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'z', 'M', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 't', 'v', 'q', 't', 'q', 'q', 'q', 't', 'q', 'q', 'q', 'v', 'q', 't', 'q', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'v', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 'l', 'q', 't', 't', 't', 'v', 'q', 't', 't', 't', 'v', 'z', 'M', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'z', 'M', 'v', 'q', 't', 't', 't', 'v', 'h', 'l', 'v', 'q', 't', 'q', 't', 'v', 'q', 't', 't', 't', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'h', 'q', 'h', 'v', 'h', 'q', 'h', 'q', 'h', 'q', 'h', 'q', 'h', 'v', 'h', 'q', 'h', 'q', 't', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'v', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'v', 'z', 'M', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 't', 'q', 'q', 't', 'v', 'q', 't', 'q', 'q', 'q', 't', 'q', 't', 'q', 'q', 't', 'v', 'q', 't', 'q', 'q', 't', 'q', 't', 't', 't', 'z', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'c', 's', 'c', 's', 'z', 'M', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'c', 'z'], ['*', '*', '*', '*', 'M', 'q', 'q', 'q', 'q', 'q', 'l', 'l', 'l', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'q', 'l', 'q', 'l', 'l', 'q', 'q', 'q', 'z', 'M', 'l', 'q', 'q', 'q', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'l', 'h', 'z', 'M', 'l', 'h', 'q', 't', 'q', 't', 'h', 'q', 't', 'q', 't', 'h', 'l', 'h', 'q', 't', 'q', 't', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'q', 't', 'q', 't', 'h', 'q', 't', 'q', 't', 'h', 'v', 'h', 'q', 't', 'q', 't', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'h', 'v', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 't', 't', 't', 't', 'z', 'M', 'v', 'l', 'v', 'q', 'h', 'v', 'h', 'v', 'h', 'q', 'v', 'q', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 't', 'l', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'v', 'q', 't', 'l', 'q', 't', 'q', 't', 'l', 'q', 'q', 't', 'l', 'q', 'l', 'q', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 't', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'h', 'l', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'v', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'h', 'q', 't', 'l', 'q', 't', 'q', 'q', 't', 'q', 'h', 'l', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'h', 'q', 'q', 't', 'v', 'q', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 'q', 'q', 't', 'l', 'q', 't', 't', 't', 'q', 'q', 'q', 't', 'v', 'q', 't', 'q', 't', 'l', 'l', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'z', 'M', 'q', 't', 'q', 'z', 'M', 'q', 't', 'q', 'z', 'M', 'q', 't', 't', 't', 'q', 't', 't', 't', 'z', 'M', 'v', 'l', 'z', 'M', 'q', 't', 'q', 't', 'z', 'M', 'q', 't', 'q', 't', 'z', 'M', 'q', 't', 'h', 'h', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'l', 'l', 'q', 'q', 't', 'q', 't', 't', 't', 'q', 't', 't', 'q', 't', 'q', 'l', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 't', 'q', 'q', 'l', 't', 't', 'q', 't', 'z', 'M', 'q', 't', 'q', 't', 'z', 'M', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 'q', 't', 'q', 'z', 'M', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 't', 't', 'q', 't', 'q', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'q', 't', 'q', 't', 'v', 'q', 't', 'q', 't', 'v', 'l', 'l', 'v', 'q', 't', 'q', 't', 'v', 'z', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'q', 't', 'q', 'q', 't', 'q', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'c', 's', 's', 'c', 'z', 'M', 'h', 'v', 'c', 'v', 'c', 'v', 'h', 'v', 'c', 'v', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'q', 'q', 't', 'q', 't', 'z', 'M', 'q', 'q', 'q', 'l', 'q', 'q', 'q', 'q', 't', 'l', 'q', 't', 'l', 'q', 'q', 'q', 'q', 'l', 'q', 'q', 'q', 'v', 'q', 't', 'q', 'q', 'z'], ['*', '*', '*', '*', 'M', 'h', 'l', 'h', 'z', 'M', 'h', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'h', 'q', 't', 'q', 't', 'h', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'l', 'h', 'z', 'M', 'l', 'h', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'q', 't', 'q', 't', 'h', 'q', 't', 'q', 't', 'h', 'v', 'h', 'q', 't', 'q', 't', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'v', 'q', 't', 'v', 'h', 'v', 'q', 'q', 'v', 'q', 't', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'c', 'c', 'c', 'c', 'z', 'M', 'h', 'v', 'c', 'v', 'c', 'c', 'v', 'l', 'l', 'v', 'c', 'v', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'l', 'h', 'z', 'M', 'l', 'h', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'l', 'h', 'z', 'M', 'h', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'q', 't', 't', 't', 'v', 'q', 't', 't', 't', 'z', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'l', 'q', 'q', 't', 'q', 'l', 'l', 'q', 't', 'q', 't', 'q', 'l', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'q', 't', 'v', 'l', 'v', 'q', 't', 'h', 'v', 'q', 't', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'z', 'M', 'l', 'q', 'q', 't', 'q', 'q', 'l', 'q', 'q', 'q', 't', 't', 'l', 'q', 'q', 't', 'q', 'v', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'q', 'q', 't', 't', 't', 't', 't', 'l', 'q', 't', 't', 't', 't', 't', 'q', 't', 'q', 't', 't', 't', 'h', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'c', 's', 'c', 'c', 'z', 'M', 'l', 'c', 'c', 'c', 'l', 'c', 'c', 'h', 'v', 'c', 'l', 'c', 'c', 'c', 'z'], ['*', '*', '*', '*', 'M', 'l', 'q', 't', 'q', 't', 'q', 't', 'l', 'q', 't', 'q', 't', 'q', 't', 'l', 'q', 't', 'q', 't', 'q', 't', 'l', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'q', 't', 't', 't', 'q', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'l', 'h', 'z', 'M', 'l', 'h', 'q', 't', 'q', 'q', 'h', 'q', 't', 'q', 'q', 'h', 'l', 'h', 'q', 't', 'q', 'q', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'q', 'v', 'q', 'v', 'l', 'v', 'q', 'v', 'h', 'z', 'M', 'q', 'q', 'q', 'q', 'z'], ['*', '*', '*', '*', 'M', 'c', 'c', 'c', 's', 'z', 'M', 'l', 'c', 'c', 'c', 'l', 'c', 'c', 'v', 'c', 'l', 'l', 'c', 'c', 'c', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'h', 'q', 'q', 't', 'h', 'q', 'q', 't', 'h', 'q', 'q', 't', 't', 't', 'q', 'q', 't', 'q', 't', 't', 'v', 'q', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'v', 'q', 't', 'v', 'h', 'v', 'q', 'v', 'q', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'l', 'l', 'v', 'h', 'v', 'l', 'l', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'q', 'q', 't', 'q', 't', 'q', 'q', 't', 'q', 'q', 't', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'q', 'q', 't', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'q', 'q', 't', 'q', 'v', 'q', 'q', 't', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'q', 'q', 'l', 'q', 'q', 'q', 't', 'q', 'q', 'q', 'q', 'q', 'l', 'l', 'q', 'q', 'z', 'M', 'q', 'q', 'q', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'l', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'l', 'h', 'z', 'M', 'l', 'h', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'q', 'q', 'q', 'q', 'q', 'q', 't', 'q', 't', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'v', 'v', 'v', 'q', 'q', 't', 'q', 'q', 'q', 'q', 'q', 't', 'q', 'q', 't', 'q', 'q', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'q', 't', 'q', 't', 't', 't', 'z', 'M', 'l', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'q', 't', 't', 't', 'q', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 'v', 'l', 'v', 'q', 't', 't', 't', 'v', 'q', 't', 'q', 't', 'v', 'q', 't', 't', 't', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'l', 'v', 'l', 'v', 'h', 'v', 'l', 'v', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'v', 'q', 't', 'q', 't', 'q', 't', 't', 'q', 'q', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 't', 't', 'z', 'M', 'v', 'h', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'v', 'h', 'v', 'h', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'l', 'h', 'l', 'h', 'z', 'M', 'l', 'h', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'v', 'h', 'v', 'h', 'v', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 'q', 't', 'q', 't', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'v', 'q', 't', 't', 'v', 'h', 'v', 'q', 't', 't', 'v', 'q', 'q', 't', 'q', 'l', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'q', 'l', 'l', 'l', 'q', 'q', 't', 'q', 'l', 'l', 'q', 'q', 't', 'z', 'M', 'q', 'l', 'l', 'q', 't', 'q', 'l', 'v', 'l', 'l', 'v', 'l', 'q', 'l', 'q', 'q', 't', 'q', 'l', 'l', 'l', 'l', 'l', 'v', 'q', 't', 'l', 'v', 'l', 'q', 'l', 'l', 'q', 'l', 'l', 'v', 'l', 'v', 'q', 't', 'q', 'l', 'q', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 'q', 't', 't', 't', 'q', 't', 'z', 'M', 'l', 'h', 'l', 'h', 'l', 'h', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 'z'], ['*', '*', '*', '*', 'M', 'h', 'v', 'h', 'v', 'z', 'M', 'h', 'v', 'h', 'v', 'z'], ['*', '*', '*', '*', 'M', 'q', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 't', 'z', 'M', 'l', 'h', 'v', 'h', 'v', 'z']]\n"
     ]
    }
   ],
   "source": [
    "#Append '*' x stepSize in front of every command\n",
    "stepSize = 4\n",
    "for index in range(len(commands)):\n",
    "    commands[index] = ['*' for i in range(stepSize)] + commands[index]\n",
    "\n",
    "print commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed:  8\n",
      "Max Len 47\n"
     ]
    }
   ],
   "source": [
    "#Remove long commands\n",
    "maxCommandLen = 50\n",
    "count = 0\n",
    "for command in commands:\n",
    "    command.append('.')\n",
    "    if len(command) > maxCommandLen:\n",
    "        commands.remove(command)\n",
    "        count += 1\n",
    "        \n",
    "print 'Removed: ', count\n",
    "print 'Max Len', max([len(command) for command in commands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are no other upper cased commands\n",
    "for command in commands:\n",
    "    for c in command:\n",
    "        if c != 'M' and c.isupper():\n",
    "            print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary for mapping to one hot vectors: oneHotMap gives letter to index\n",
    "commandList = ['*','M','m','l','h','v','z','c','s','q','t','a','.']\n",
    "oneHotMap = {}\n",
    "for i, c in enumerate(commandList):\n",
    "    oneHotMap[c] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ont hot commands\n",
    "oneHotCommands = np.zeros((len(commands), maxCommandLen, len(commandList)))\n",
    "\n",
    "noOfFeatures = len(commandList)\n",
    "noOfCommands = len(oneHotCommands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#populate X train\n",
    "for i, command in enumerate(commands):\n",
    "    for j, c in enumerate(command):\n",
    "        oneHotCommands[i, j, oneHotMap[c]] = 1\n",
    "        \n",
    "print oneHotCommands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 50, 4, 13) (113, 50, 13)\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((noOfCommands, maxCommandLen, stepSize, noOfFeatures))\n",
    "Y = np.zeros((noOfCommands, maxCommandLen, noOfFeatures))\n",
    "\n",
    "print X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.Sequential()\n",
    "\n",
    "# model.add(LSTM(input_shape = (3,noOfFeatures), output_dim = 50))\n",
    "# model.add(LSTM(output_dim = 100))\n",
    "# model.add(Dense(output_dim = noOfFeatures))\n",
    "# model.add(Activation('linear'))\n",
    "# model.add(loss = 'mse', optimizer = 'rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 50, 4, 13)\n"
     ]
    }
   ],
   "source": [
    "print X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer lstm_36: expected ndim=3, found ndim=4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-72ab43a1ab6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxCommandLen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepSize\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnoOfFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/prayansh/Documents/code/ml/tf-env/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/prayansh/Documents/code/ml/tf-env/lib/python2.7/site-packages/keras/layers/recurrent.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/prayansh/Documents/code/ml/tf-env/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/prayansh/Documents/code/ml/tf-env/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    470\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer lstm_36: expected ndim=3, found ndim=4"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(stepSize,  noOfFeatures)))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(noOfFeatures))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(X)):\n",
    "    hist = model.fit(X[idx], Y[idx], batch_size=500, nb_epoch=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.predict(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]\n",
      " [0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      "  0.07692308]]\n"
     ]
    }
   ],
   "source": [
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
